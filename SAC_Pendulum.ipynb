{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers, losses\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "import tensorflow_probability as tfp"
      ],
      "metadata": {
        "id": "s0W3JZi84Dry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class experience_memory():\n",
        "\n",
        "    def __init__(self, buffer_capacity, batch_size, state_dim, action_dim):\n",
        "        # Number of \"experiences\" to store at max\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Num of tuples to train on.\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Its tells us num of times record() was called.\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # Instead of list of tuples as the exp.replay concept go\n",
        "        # We use different np.arrays for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, state_dim), dtype=np.float32)\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, action_dim), dtype=np.float32)\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1), dtype=np.float32)\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, state_dim), dtype=np.float32)\n",
        "        self.done_buffer = np.zeros((self.buffer_capacity, 1), dtype=np.float32)\n",
        "\n",
        "    # Takes (s,a,r,s') obervation tuple as input\n",
        "    def record(self, obs_tuple):\n",
        "        # Set index to zero if buffer_capacity is exceeded,\n",
        "        # replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "\n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.action_buffer[index] = obs_tuple[1]\n",
        "        self.reward_buffer[index] = obs_tuple[2]\n",
        "        self.next_state_buffer[index] = obs_tuple[3]\n",
        "        self.done_buffer[index] = obs_tuple[4]\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "class Pendulum():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(\"Pendulum-v1\")\n",
        "        self.batch_size = 64\n",
        "        self.max_memory_size = 1000000\n",
        "\n",
        "        self.state_dim = 3\n",
        "        self.action_dim = 1\n",
        "\n",
        "        self.gamma = 1\n",
        "        self.tau = 0.005\n",
        "\n",
        "        # for the entropy regulization\n",
        "        self.temp = 0.2\n",
        "\n",
        "        self.lower_action_bound = -2\n",
        "        self.upper_action_bound = 2\n",
        "\n",
        "        self.buffer = experience_memory(self.max_memory_size, self.batch_size, self.state_dim, self.action_dim)\n",
        "\n",
        "        \n",
        "        \n",
        "    \n",
        "        self.critic_1 = self.get_critic()\n",
        "        self.target_critic_1 = self.get_critic()\n",
        "        self.target_critic_1.set_weights(self.critic_1.get_weights())\n",
        "\n",
        "        self.critic_2 = self.get_critic()\n",
        "        self.target_critic_2 = self.get_critic()\n",
        "        self.target_critic_2.set_weights(self.critic_2.get_weights())\n",
        "        \n",
        "        self.actor = self.get_actor()\n",
        "            \n",
        "        self.critic_lr = 0.003\n",
        "        self.critic_optimizer_1 = tf.keras.optimizers.Adam(self.critic_lr)\n",
        "        self.critic_optimizer_2 = tf.keras.optimizers.Adam(self.critic_lr)\n",
        "\n",
        "        self.actor_lr = 0.003\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(self.actor_lr)\n",
        "      \n",
        "   \n",
        "        self.lr_decay = 0.9999\n",
        "\n",
        "        self.update = 1\n",
        "       \n",
        "    def save_model(self):\n",
        "        self.critic.save('./Models/critic.h5')\n",
        "        \n",
        "        self.actor.save('./Models/actor.h5')\n",
        "    def update_lr(self):\n",
        "        self.critic_lr = self.critic_lr * self.lr_decay\n",
        "        self.critic_optimizer_1 = tf.keras.optimizers.Adam(self.critic_lr)\n",
        "        self.critic_optimizer_2 = tf.keras.optimizers.Adam(self.critic_lr)\n",
        "        \n",
        "        self.actor_lr = self.actor_lr * self.lr_decay\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(self.actor_lr)\n",
        "\n",
        "    def get_critic(self):\n",
        "      input_state = keras.Input(shape =(self.state_dim,))\n",
        "      input_action = keras.Input(shape =(self.action_dim,))\n",
        "      input = tf.concat([input_state, input_action], axis = 1 )\n",
        "      d1 = layers.Dense(256, activation = 'relu')(input)\n",
        "      d2 = layers.Dense(256, activation = 'relu')(d1)\n",
        "      out = layers.Dense(1)(d2)\n",
        "      model = keras.Model(inputs =  [input_state, input_action], outputs = out)\n",
        "      return model\n",
        "\n",
        "    \n",
        "    def get_actor(self):\n",
        "      input = keras.Input(shape = (self.state_dim,))\n",
        "\n",
        "      d1 = layers.Dense(256, activation = 'relu')(input)\n",
        "      d2 = layers.Dense(256, activation = 'relu')(d1)\n",
        "\n",
        "      mu = layers.Dense(self.action_dim)(d2)\n",
        "      log_std = layers.Dense(self.action_dim)(d2)\n",
        "\n",
        "      model = keras.Model(inputs = input, outputs = [mu, log_std])\n",
        "     \n",
        "      return model\n",
        "    \n",
        "    def transform_actor(self, mu, log_std):\n",
        "      clip_log_std = tf.clip_by_value(log_std, -20,2)\n",
        "\n",
        "      std =  tf.exp(clip_log_std)\n",
        "      \n",
        "      dist = tfp.distributions.Normal(mu, std,allow_nan_stats=False)\n",
        "\n",
        "      action_ = dist.sample()\n",
        "      action = tf.tanh(action_)\n",
        "\n",
        "      log_pi = dist.log_prob(action_)\n",
        "      log_pi_a = log_pi - tf.reduce_sum(tf.math.log((1-action**2) + 1e-6), axis = 1, keepdims = True)\n",
        "      action = self.upper_action_bound*action\n",
        "      return action, log_pi_a\n",
        "\n",
        "    @tf.function\n",
        "    def update_critic(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch):\n",
        "        mu, std = self.actor(next_state_batch)\n",
        "        pi_a ,log_pi_a = self.transform_actor(mu, std)\n",
        "                                              \n",
        "        target_1 = self.target_critic_1([next_state_batch, pi_a])\n",
        "        target_2 = self.target_critic_1([next_state_batch, pi_a])\n",
        "        \n",
        "        target_vals =  tf.minimum(target_1, target_2)\n",
        "\n",
        "        # soft target\n",
        "      \n",
        "        y = tf.stop_gradient(reward_batch + (1-done_batch)* self.gamma*(target_vals - self.temp*log_pi_a))\n",
        "\n",
        "        with tf.GradientTape() as tape1:\n",
        "            \n",
        "            critic_value_1 = self.critic_1([state_batch, action_batch])\n",
        "            \n",
        "            critic_1_loss = losses.MSE(y,critic_value_1 )\n",
        "\n",
        "        with tf.GradientTape() as tape2:\n",
        "            \n",
        "            critic_value_2 = self.critic_2([state_batch, action_batch])\n",
        "            \n",
        "            critic_2_loss = losses.MSE(y,critic_value_2)\n",
        "\n",
        "        \n",
        "        critic_1_grad = tape1.gradient(critic_1_loss, self.critic_1.trainable_variables)   \n",
        "        self.critic_optimizer_1.apply_gradients(zip(critic_1_grad, self.critic_1.trainable_variables))\n",
        "\n",
        "        critic_2_grad = tape2.gradient(critic_2_loss, self.critic_2.trainable_variables)   \n",
        "        self.critic_optimizer_2.apply_gradients(zip(critic_2_grad, self.critic_2.trainable_variables))\n",
        "\n",
        "\n",
        "       \n",
        "\n",
        "    @tf.function\n",
        "    def update_actor(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch):\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            mu,std = self.actor(state_batch)\n",
        "            pi_a, log_pi_a = self.transform_actor(mu, std)\n",
        "            critic_value_1 = self.critic_1([state_batch, pi_a])\n",
        "            critic_value_2 = self.critic_2([state_batch, pi_a])\n",
        "            min_critic = tf.minimum(critic_value_1, critic_value_2)\n",
        "\n",
        "            soft_q = min_critic - self.temp * log_pi_a\n",
        "\n",
        "            # for maximize add a minus '-tf.math.reduce_mean(soft_q)'\n",
        "            actor_loss = -tf.math.reduce_mean(soft_q)\n",
        "            \n",
        "        actor_grad = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
        "        \n",
        "        self.actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, self.actor.trainable_variables)\n",
        "        )\n",
        "     \n",
        "    def learn(self,episode):\n",
        "        # get sample\n",
        "\n",
        "        record_range = min(self.buffer.buffer_counter, self.buffer.buffer_capacity)\n",
        "\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        state_batch = tf.convert_to_tensor(self.buffer.state_buffer[batch_indices])\n",
        "        action_batch = tf.convert_to_tensor(self.buffer.action_buffer[batch_indices])\n",
        "        reward_batch = tf.convert_to_tensor(self.buffer.reward_buffer[batch_indices])\n",
        "        next_state_batch = tf.convert_to_tensor(self.buffer.next_state_buffer[batch_indices])\n",
        "\n",
        "        done_batch = tf.convert_to_tensor(self.buffer.done_buffer[batch_indices])\n",
        "        \n",
        "        self.update_critic(state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
        "\n",
        "        if (episode % self.update == 0 and episode != 0):\n",
        "            self.update_actor(state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def update_target(self, target_weights, weights):\n",
        "        for (a,b) in zip(target_weights, weights):\n",
        "            a.assign(self.tau *b + (1-self.tau) *a)"
      ],
      "metadata": {
        "id": "qlvOld9Kj4KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MC = Pendulum()\n",
        "\n",
        "ep_reward_list = []\n",
        "# To store average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "num_episode = 500\n",
        "decay = 0.9999\n",
        "for ep in range(num_episode):\n",
        "    \n",
        "    done = False\n",
        "    state = MC.env.reset()\n",
        "    state = tf.expand_dims(tf.convert_to_tensor(state),0)\n",
        "    \n",
        "    #state = np.reshape(state, [1,MC.state_dim])\n",
        "    episodic_reward = 0\n",
        "    t_counter = 0\n",
        "\n",
        "    #if (ep % 100 == 0 and ep != 0):\n",
        "        #MC.run_MC()\n",
        "    while(True):\n",
        "        mu, log_std = MC.actor(state)\n",
        "        action_,_ = MC.transform_actor(mu, log_std)\n",
        "        \n",
        "        action = action_[0].numpy()\n",
        "       \n",
        "        new_state, reward, done, info = MC.env.step(action)\n",
        "        \n",
        "        new_state = tf.expand_dims(tf.convert_to_tensor(new_state.reshape(MC.state_dim)),0)\n",
        "       \n",
        "        #new_state = np.reshape(new_state, [1,MC.state_dim])\n",
        "        episodic_reward += reward\n",
        "        \n",
        "        MC.buffer.record((state,action,reward, new_state, done))\n",
        "       \n",
        "        MC.learn(ep)\n",
        "    \n",
        "        MC.update_target(MC.target_critic_1.variables, MC.critic_1.variables)\n",
        "                    \n",
        "        MC.update_target(MC.target_critic_2.variables, MC.critic_2.variables)\n",
        "       \n",
        "\n",
        "        state = new_state\n",
        "        t_counter +=1\n",
        "        if (done):\n",
        "            break\n",
        "    ep_reward_list.append(episodic_reward)\n",
        "    # Mean of last 40 episodes\n",
        "    avg_reward = np.mean(ep_reward_list[-20:])\n",
        "    print(\"Episode * {} * AVG Reward is ==> {}, actor_lr ==> {}\".format(ep, avg_reward,MC.actor_lr))\n",
        "    avg_reward_list.append(avg_reward)\n",
        "# Plotting graph\n",
        "# Episodes versus Avg. Rewards\n",
        "plt.plot(avg_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QKICf78FnI-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = np.ones(2) -0.5\n",
        "\n",
        "u_min = np.array([0,4])\n",
        "u_max = np.array([2,8])\n",
        "c = tf.expand_dims(tf.convert_to_tensor(c),0)\n",
        "print(c)\n",
        "b = tf.expand_dims(tf.convert_to_tensor(u_max- u_min, dtype = tf.float64),0)\n",
        "a = tf.expand_dims(tf.convert_to_tensor(u_min, dtype = tf.float64),0)\n",
        "print(b)\n",
        "print(b*c + a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Syq2EbWW1lT3",
        "outputId": "e02b7bed-e621-4ed5-b4f9-f759a1d1e36a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[0.5 0.5]], shape=(1, 2), dtype=float64)\n",
            "tf.Tensor([[2. 4.]], shape=(1, 2), dtype=float64)\n",
            "tf.Tensor([[1. 6.]], shape=(1, 2), dtype=float64)\n"
          ]
        }
      ]
    }
  ]
}